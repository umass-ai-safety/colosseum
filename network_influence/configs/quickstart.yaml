experiment:
  # Small, cheap configuration intended for new users to validate setup end-to-end.
  #
  # This should finish quickly and produce `outputs/.../<timestamp>/runs/...` artifacts
  # plus a `summary_mean.png` plot via `plots.generate_all`.
  tag: network_influence_quickstart
  output_dir: experiments/network_influence/outputs/network_influence_quickstart

  # The runner maps these onto `simulation.max_planning_rounds` / `simulation.max_conversation_steps`.
  planning_rounds: 1
  max_conversation_steps: 1
  max_concurrent_runs: 1

  # One run per setting.
  seeds: [1]

  adversary:
    strategy: random  # random | highest_degree | center
    force_seed: true
    victim_assignment: shared  # distinct | shared
    misinfo_frequency: once  # once | always

  info:
    # Change these labels to match your environment semantics. They are used in the survey + judge.
    code_prefix: PREF_CODE
    true_preference: no_trip
    false_preference: on_trip

  judge:
    # The judge uses the OpenAI client (OPENAI_API_KEY required).
    model: gpt-4o-mini
    params:
      max_tokens: 256
      temperature: 0.0

  sweeps:
    - name: quickstart
      topologies: [path]
      num_agents: [6]
      adversary_counts: [0, 1]

environment:
  name: MeetingSchedulingEnvironment
  assignment_filling: false
  num_meetings: 3
  timeline_length: 12
  min_participants: 2
  max_participants: 4
  soft_meeting_ratio: 0.6

communication_network:
  # `topology` and `num_agents` are overwritten by the sweep values per-run.
  topology: path
  num_agents: 6
  consolidate_channels: true
  # used by: erdos_renyi
  edge_prob: 0.6
  # used by: watts_strogatz (k must be even and < num_agents)
  k: 2
  rewire_prob: 0.3
  # used by: barabasi_albert (1 <= m < num_agents)
  m: 3

llm_models:
  # Use any provider/model you have access to. The judge is configured separately above.
  - label: openai-gpt-4o-mini
    llm:
      provider: openai
      openai:
        model: gpt-4o-mini
        params:
          max_tokens: 800
          temperature: 0.7
